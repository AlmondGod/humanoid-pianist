# Soft Actor-Critic (with DroQ)

## Goal

Train a policy in high-dimensional continuous actio spaces that does nto get stuck at local minima and is not as sensitive to hyperparameters as past RL algorithms

## Background

Soft Actor-Critic (SAC) is off-policy, meaning it learns from data not necessarily generated by the current policy. In practice, we often use SAC online (interacting with an environement) and store data in a replay buffer. SAC combines techniques from:
1. DDPG: offline actor-critic architecture (but has a deterministic actor)
2. Soft Q-Learning: maximizes entropy to encourage exploration

## Architecture

We use typical actor-critic architecture, but now we add to our critic:
1. Using the minimum of 2 Q functions to reduce Q overestimation bias
2. use target networks for more stable TD learning
2. Include an entropy term in the values which discourages deterministic actions and thus encourages exploration of the state space.

The `robopianist-rl` implementation (in `architecture/sac.py`) uses:

1. **Actor Network**: MLP with 3x256 hidden layers, predicts normal dist for each action dimension, `TanhMultivariateNormalDiag` distribution for bounded actions

2. **Critic Network**: Two Q-networks with 3x256 hidden layers, Layer normalization and dropout (0.01) for regularization (DroQ)

## Benefits

1. Off-policy learning allows reuse of past experience (sample efficient)
2. Double Q-learning reduces bellman-induced overestimation bias, DroQ improves generalization
3. Maximum entropy so gets stuck at local minima less

## Getting Training to work

1. **Hyperparameters that worked for me with no fingering annotations**:
   - Learning rates: ~3e-4 for all networks
   - Batch size: 256
   - Target network update rate (tau): 0.01
   - Initial temperature: 1.2

4. **Monitoring**:
   - Track Q-values to detect overestimation
   - Ensure consistent relatively high entropy and gradually increasing Qs after beginning stabilization in training